Index: beam/src/main/kotlin/eventstream/beam/pipeline/transform/csv/BeamParquet.kt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/beam/src/main/kotlin/eventstream/beam/pipeline/transform/csv/BeamParquet.kt b/beam/src/main/kotlin/eventstream/beam/pipeline/transform/csv/BeamParquet.kt
--- a/beam/src/main/kotlin/eventstream/beam/pipeline/transform/csv/BeamParquet.kt	
+++ b/beam/src/main/kotlin/eventstream/beam/pipeline/transform/csv/BeamParquet.kt	(date 1699573382024)
@@ -13,16 +13,13 @@
 import org.apache.parquet.hadoop.metadata.CompressionCodecName
 
 /*
-Path - cd /d/Code/Kotlin/projects/eventstream/data/output/beam
 
 Metadata
-parq  output-00000-of-00005.parquet
+parq input.parquet
 
 Schema
-parq  output-00000-of-00005.parquet  --schema
+parq input.parquet --schema
 
-Top n Rows
-parq output-00000-of-00005.parquet --head 10
 
  */
 object BeamParquet {
Index: beam/src/main/kotlin/eventstream/beam/transformations/CSV.kt
===================================================================
diff --git a/beam/src/main/kotlin/eventstream/beam/transformations/CSV.kt b/beam/src/main/kotlin/eventstream/beam/transformations/CSV.kt
deleted file mode 100644
--- a/beam/src/main/kotlin/eventstream/beam/transformations/CSV.kt	
+++ /dev/null	
@@ -1,34 +0,0 @@
-package eventstream.beam.transformations
-
-import eventstream.beam.BeamTransformation
-import eventstream.beam.TRANSFORMATION
-import eventstream.beam.effects.WriteCollection
-import org.apache.beam.sdk.transforms.MapElements
-import org.apache.beam.sdk.values.PCollection
-import org.apache.beam.sdk.values.TypeDescriptor
-
-data class UppercaseTransformParams(val output: String?, val writeFiles: Boolean?)
-
-object CsvTransformation {
-
-    object ConvertLinesUppercase :
-        BeamTransformation<UppercaseTransformParams, PCollection<String>, PCollection<String>>() {
-        override val transformationType = TRANSFORMATION.CAPITALIZE_LINE
-        override fun apply(input: PCollection<String>, params: UppercaseTransformParams): PCollection<String> {
-            val upperCaseLines = input
-                .apply(
-                    "Uppercase Lines", MapElements.into(TypeDescriptor.of(String::class.java))
-                        .via(StringTransform.StringUpperCaseFunction())
-                )
-
-            // Optionally write to output if specified in params
-            params.output?.takeIf { params.writeFiles ?: false }?.let { outputPath ->
-                WriteCollection().outputCollections(upperCaseLines, outputPath, ".txt")
-            }
-
-            return upperCaseLines // Return the transformed PCollection
-        }
-
-    }
-
-}
\ No newline at end of file
Index: beam/src/main/kotlin/eventstream/beam/transformations/DoFn.kt
===================================================================
diff --git a/beam/src/main/kotlin/eventstream/beam/transformations/DoFn.kt b/beam/src/main/kotlin/eventstream/beam/transformations/DoFn.kt
deleted file mode 100644
--- a/beam/src/main/kotlin/eventstream/beam/transformations/DoFn.kt	
+++ /dev/null	
@@ -1,54 +0,0 @@
-package eventstream.beam.transformations
-
-import eventstream.beam.BeamEntity
-import eventstream.beam.SerializableEntity
-import org.apache.beam.sdk.coders.Coder
-import org.apache.beam.sdk.transforms.DoFn
-import org.apache.beam.sdk.transforms.ParDo
-import org.apache.beam.sdk.values.PCollection
-import org.checkerframework.checker.initialization.qual.Initialized
-import org.checkerframework.checker.nullness.qual.UnknownKeyFor
-
-
-class EntityDoFn<T>(
-    private val serializeFn: (String) -> T?
-) : DoFn<String, T>() where T : BeamEntity {
-    @ProcessElement
-    fun processElement(@Element line: String, out: OutputReceiver<T>) {
-        val entity: T? = serializeFn(line)
-        if (entity != null) {
-            out.output(entity)
-        }
-    }
-}
-
-// Define a separate DoFn class
-class ConvertCsvToEntityFn<T : BeamEntity>(
-    private val entity: SerializableEntity<T>,
-    private val coder: Coder<T?>
-) : DoFn<String, T?>() {
-
-    @ProcessElement
-    fun processElement(c: ProcessContext, receiver: OutputReceiver<T?>) {
-        val line = c.element()
-        val result: T? = try {
-            entity.serializeFromCsvLine(line)
-        } catch (e: Exception) {
-            null // Handle error appropriately
-        }
-        if (result != null) {
-            receiver.output(result)
-        }
-    }
-}
-
-// Modify your applyCsvToEntityTransformation function to use the new DoFn class
-fun <T : BeamEntity> applyCsvToEntityTransformation(
-    input: PCollection<String>,
-    entity: SerializableEntity<T>,
-    coder: Coder<T?>
-): @UnknownKeyFor @Initialized PCollection<T?> {
-    val doFn = ConvertCsvToEntityFn(entity, coder)
-    val parsedEntityPCollection = input.apply("Parse CSV Lines to Entities", ParDo.of(doFn))
-    return parsedEntityPCollection.setCoder(coder as Coder<T?>)
-}
Index: beam/src/main/kotlin/eventstream/beam/transformations/entities/csventity.kt
===================================================================
diff --git a/beam/src/main/kotlin/eventstream/beam/transformations/entities/csventity.kt b/beam/src/main/kotlin/eventstream/beam/transformations/entities/csventity.kt
deleted file mode 100644
--- a/beam/src/main/kotlin/eventstream/beam/transformations/entities/csventity.kt	
+++ /dev/null	
@@ -1,42 +0,0 @@
-package eventstream.beam.transformations.entities
-
-import eventstream.beam.BeamEntity
-import eventstream.beam.SerializableEntity
-import eventstream.beam.transformations.BeamSerialization
-import eventstream.beam.transformations.EntityDoFn
-import io.github.oshai.kotlinlogging.KotlinLogging
-import org.apache.beam.sdk.coders.NullableCoder
-import org.apache.beam.sdk.transforms.DoFn
-import org.apache.beam.sdk.transforms.ParDo
-import org.apache.beam.sdk.values.PCollection
-
-fun <T : BeamEntity> applyCsvToEntityTransformation(
-    input: PCollection<String>,
-    entity: SerializableEntity<T>
-): PCollection<T> {
-    val avroEntityCoder = entity.getCoder()
-    val parsedEntityPCollection = input.apply(
-        "Parse CSV Lines to Entities",
-        ParDo.of(EntityDoFn(entity::serializeFromCsvLine)) // Pass method reference directly.
-    ).setCoder(NullableCoder.of(avroEntityCoder))
-
-    val logger = KotlinLogging.logger {}
-
-    // If you have further processing to do that requires a non-null PCollection, apply a filter transform.
-    val nonNullEntityPCollection = parsedEntityPCollection.apply(
-        "Filter Null Entities",
-        ParDo.of(object : DoFn<T?, T>() {
-            @ProcessElement
-            fun processElement(@Element entity: T?, out: OutputReceiver<T>) {
-                if (entity != null) {
-                    out.output(entity)
-                    BeamSerialization.SerializeEntityFromCSVLines.logger.info { "Successfully parsed: ${entity.toString()}" }
-
-                }
-            }
-        })
-    ).setCoder(avroEntityCoder)
-
-    // Return the non-null PCollection.
-    return nonNullEntityPCollection
-}
\ No newline at end of file
Index: beam/src/main/kotlin/eventstream/beam/transformations/StringTransform.kt
===================================================================
diff --git a/beam/src/main/kotlin/eventstream/beam/transformations/StringTransform.kt b/beam/src/main/kotlin/eventstream/beam/transformations/StringTransform.kt
deleted file mode 100644
--- a/beam/src/main/kotlin/eventstream/beam/transformations/StringTransform.kt	
+++ /dev/null	
@@ -1,12 +0,0 @@
-package eventstream.beam.transformations
-
-import org.apache.beam.sdk.transforms.SimpleFunction
-
-object StringTransform {
-
-    class StringUpperCaseFunction : SimpleFunction<String, String>() {
-        override fun apply(input: String): String {
-            return input.uppercase()
-        }
-    }
-}
\ No newline at end of file
Index: beam/src/main/kotlin/eventstream/beam/transformations/BeamSerialization.kt
===================================================================
diff --git a/beam/src/main/kotlin/eventstream/beam/transformations/BeamSerialization.kt b/beam/src/main/kotlin/eventstream/beam/transformations/BeamSerialization.kt
deleted file mode 100644
--- a/beam/src/main/kotlin/eventstream/beam/transformations/BeamSerialization.kt	
+++ /dev/null	
@@ -1,99 +0,0 @@
-package eventstream.beam.transformations
-
-import eventstream.beam.BeamEntity
-import eventstream.beam.BeamTransformation
-import eventstream.beam.SerializableEntity
-import eventstream.beam.TRANSFORMATION
-import io.github.oshai.kotlinlogging.KotlinLogging
-import org.apache.beam.sdk.coders.NullableCoder
-import org.apache.beam.sdk.transforms.*
-import org.apache.beam.sdk.values.KV
-import org.apache.beam.sdk.values.PCollection
-import org.apache.beam.sdk.values.TypeDescriptors
-
-fun interface CsvLineSerializer<T> {
-    fun serializeFromCsvLine(line: String): T?
-}
-
-class EntityOptions(
-    val inputs: List<String>,
-    val serializer: SerializableEntity<out BeamEntity>? // Use the out-projected type for flexibility
-)
-
-data class SerializationParams(val serializer: SerializableEntity<BeamEntity>)
-
-class BeamSerialization {
-    object SerializeEntityFromCSVLines :
-        BeamTransformation<SerializationParams, PCollection<String>, PCollection<BeamEntity>>() {
-        private val logger = KotlinLogging.logger {}
-        override val transformationType = TRANSFORMATION.SERIALIZE_ENTITY
-        override fun apply(input: PCollection<String>, params: SerializationParams): PCollection<BeamEntity> {
-            return applyCsvToEntityTransformation(input, params.serializer)
-        }
-
-        fun <T : BeamEntity> applyCsvToEntityTransformation(
-            input: PCollection<String>,
-            entity: SerializableEntity<T>
-        ): PCollection<T> {
-            val avroEntityCoder = entity.getCoder()
-            val parsedEntityPCollection = input.apply(
-                "Parse CSV Lines to Entities",
-                ParDo.of(EntityDoFn(entity::serializeFromCsvLine)) // Pass method reference directly.
-            ).setCoder(NullableCoder.of(avroEntityCoder))
-
-            // If you have further processing to do that requires a non-null PCollection, apply a filter transform.
-            val nonNullEntityPCollection = parsedEntityPCollection.apply(
-                "Filter Null Entities",
-                ParDo.of(object : DoFn<T?, T>() {
-                    @ProcessElement
-                    fun processElement(@Element entity: T?, out: OutputReceiver<T>) {
-                        if (entity != null) {
-                            out.output(entity)
-                            logger.info { "Successfully parsed: ${entity.toString()}" }
-
-                        }
-                    }
-                })
-            ).setCoder(avroEntityCoder)
-
-            // Return the non-null PCollection.
-            return nonNullEntityPCollection
-        }
-
-        fun <T : BeamEntity> analyzeGenericCols(
-            entityCollection: PCollection<T>,
-            columns: List<String>,
-            // getFieldValue: (T, String) -> Any? // Generic getFieldValue passed as a lambda
-        ) {
-            columns.forEach { columnName ->
-                val fieldCounts: PCollection<KV<String, Long>> = entityCollection
-                    .apply(
-                        "Extract $columnName",
-                        MapElements.into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.longs()))
-                            .via(SerializableFunction<T, KV<String, Long>> { entity ->
-                                KV.of(entity.getFieldValue(columnName).toString(), 1L)
-                            })
-                    )
-                    .apply(
-                        "Filter nulls for $columnName",
-                        Filter.by(SerializableFunction<KV<String, Long>, Boolean> { kv -> kv != null })
-                    )
-                    .apply("Count Per $columnName", Count.perKey())
-
-                // Log the counts for each column
-                fieldCounts.apply(
-                    "Log $columnName Counts", ParDo.of(object : DoFn<KV<String, Long>, Void>() {
-                        @ProcessElement
-                        fun processElement(@Element count: KV<String, Long>, context: ProcessContext) {
-                            println("===\n$columnName: ${count.key}, Count: ${count.value}")
-                            // ... Additional logging if necessary
-                        }
-                    })
-                )
-            }
-        }
-
-    }
-
-
-}
